{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Biological Signals Analysis - Week 10 Exercise\n",
    "# Neural Decoding\n",
    "### Jan. 10, 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Neural Decoding\n",
    "\n",
    "Decoding in neuroscience refers to the interpretation of neural signals to infer the original sensory stimulus or the intended motor action. This field holds significant importance in understanding brain function and developing brain-computer interfaces.\n",
    "In this session, we'll explore the principles and applications of Linear Stimulus Reconstruction (LSR) in neural coding. LSR is a method used to predict the external stimulus that caused a neuron's spike activity. We will discuss its mathematical foundation, introduce the concepts of linear filtering, and the role of least-squares regression in decoding neural signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background: Bayes' Theorem\n",
    "\n",
    "#### Part I: Bayesian Framework for Neural Decoding\n",
    "\n",
    "Bayes' theorem is a foundational principle in probability theory and statistics, and it is crucial for understanding the process of neural decoding. In the context of neuroscience, neural decoding involves interpreting the spikes or firing rates from neurons in response to stimuli. Let's break down Bayes' theorem and its key components, which we will use throughout our exploration of neural decoding:\n",
    "\n",
    "- **$ r $**: Represents the firing rate or the response of our neural system.\n",
    "- **$ s $**: Denotes the stimulus.\n",
    "- **$ P(s) $**: The probability of a stimulus $ s $ being presented.\n",
    "- **$ P(r) $**: The probability of receiving a response $ r $.\n",
    "- **$ P(r, s) $**: The joint probability of both stimulus $ s $ and response $ r $ occurring.\n",
    "- **$ P(r|s) $**: The probability of evoking response $ r $ given that stimulus $ s $ was presented (the encoding probability).\n",
    "- **$ P(s|r) $**: The probability of stimulus $ s $ being presented given the response $ r $ (the decoding probability).\n",
    "\n",
    "Bayes' theorem provides a way to update our probability estimates for a hypothesis as we gain more evidence. It is written as:\n",
    "\n",
    "$ P(s|r) = \\frac{P(r|s)P(s)}{P(r)} $\n",
    "\n",
    "In words, Bayes' theorem states that the probability of the stimulus given the response is equal to the probability of the response given the stimulus times the probability of the stimulus, all divided by the probability of the response.\n",
    "\n",
    "This theorem allows us to reverse the conditional probability, thereby moving from the probability of observing the data given a known hypothesis (the encoding model) to the probability of the hypothesis given the observed data (the decoding model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability of the stimulus given the response (P(s|r)) is: 0.47\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define probabilities\n",
    "P_s = 0.2  # P(s) - the prior probability of the stimulus\n",
    "P_r_given_s = 0.7  # P(r|s) - the probability of the response given the stimulus\n",
    "P_r = 0.3  # P(r) - the probability of the response\n",
    "\n",
    "# Apply Bayes' theorem to find P(s|r)\n",
    "P_s_given_r = (P_r_given_s * P_s) / P_r\n",
    "\n",
    "print(f\"The probability of the stimulus given the response (P(s|r)) is: {P_s_given_r:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we have chosen arbitrary values for $ P(s) $, $ P(r|s) $, and $ P(r) $. By plugging these into Bayes' theorem, we calculate the posterior probability $ P(s|r) $, which gives us an updated belief about the occurrence of the stimulus given the response we've observed. This process is at the heart of neural decoding, where we interpret neuronal firing rates to determine the most likely stimulus that caused them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Part 2: The Linear Filter Model\n",
    "\n",
    "\"To reconstruct a stimulus from spike trains, we assume a linear relationship between the stimulus and the spikes. The model is described by the equation:\n",
    "\n",
    "\\[ x(t) = x_0 + \\sum_f k(t - t^f) \\]\n",
    "\n",
    "Here, $ x(t) $ represents the predicted stimulus at time $ t $, $ x_0 $ is a constant representing the baseline stimulus level, $ k $ is the temporal filter, and $ t^f $ are the spike times preceding $ t $.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 3: Temporal Filtering\n",
    "\n",
    "\"The temporal filter $ k $ is essential to our model. It characterizes the influence of a single spike at time $ t^f $ on the stimulus estimate at time $ t $. This filter must be determined based on the data, and its shape can provide insights into the temporal dynamics of the stimulus encoding.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 4: Determining the Optimal Linear Estimator\n",
    "\n",
    "\"To find the best filter $ k $, we apply an optimization technique known as the Optimal Linear Estimator (OLE). The OLE is derived using a least-squares approach, fitting the filter to minimize the difference between the predicted and observed stimuli.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 5: Least-Squares Regression\n",
    "\n",
    "\"In practice, we discretize time and the filter to apply least-squares regression. This statistical method finds the coefficients (in our case, the filter values) that minimize the sum of the squares of the residuals, the differences between observed and predicted values. The resulting filter provides the best linear prediction of the stimulus, given the spike data.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 6: From Theory to Practice\n",
    "\n",
    "\"Applying this method requires careful preprocessing of the spike data, including aligning spike times with the stimulus representation and potentially smoothing the data to account for noise and variability.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 7: Regression in Action\n",
    "\n",
    "\"We'll walk through an example using simulated neural data. Our Python implementation will demonstrate how to discretize spike times, apply the temporal filter, perform least-squares regression, and finally, visualize the reconstructed stimulus compared to the original.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Part 8: Implications and Applications\n",
    "\n",
    "\"Understanding and applying LSR has profound implications in fields such as neuroprosthetics, where decoding neural signals can enable the control of prosthetic limbs or restore sensory experiences. Additionally, LSR techniques contribute to our fundamental understanding of how sensory information is processed in the brain.\"\n",
    "\n",
    "---\n",
    "\n",
    "#### Conclusion\n",
    "\n",
    "\"We've covered the theoretical framework of Linear Stimulus Reconstruction and its significance in neuroscience research. LSR is a powerful tool that allows us to peek into the brain's encoding scheme and translate neural activity into meaningful information.\"\n",
    "\n",
    "---\n",
    "\n",
    "This structured lecture provides a foundational understanding of linear stimulus reconstruction, encompassing the theoretical aspects, mathematical derivations, and practical implications in neuroscience. The content is designed to be comprehensive, educating PhD students about the intricate relationship between neural spikes and stimuli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Linear Stimulus Reconstruction\n",
    "Linear Stimulus Reconstruction is a technique used to predict the stimulus that caused a series of spikes in neural data. This method is critical for understanding how neurons encode information about stimuli.\n",
    "The core of this reconstruction technique is the use of a linear filter, which is applied to the observed spike times. The filter is designed to optimally predict the stimulus that caused these spikes.\n",
    "For linear stimulus reconstruction, we predict the stimulus $ x(t) $ by linearly filtering observed spike times. The goal is to determine the optimal filter shape, $ k $, for the best linear estimation of the stimulus.\n",
    "\n",
    "To reconstruct a stimulus from spike trains, we assume a linear relationship between the stimulus and the spikes. The model is described by the equation:\n",
    "\n",
    "$ x(t) = x_0 + \\sum_f k(t - t^f) $\n",
    "\n",
    "Here, $ x(t) $ represents the predicted stimulus at time $ t $, $ x_0 $ is a constant representing the baseline stimulus level, $ k $ is the temporal filter, and \\( t^f $ are the spike times preceding \\( t \\).\"\n",
    "\n",
    "#### Least-Squares Regression for Optimal Linear Estimator (OLE)\n",
    "\n",
    "To determine the filter $ k $, we perform a least-squares regression of the spike data onto the stimulus. This involves discretizing both time and the temporal filter to solve the optimization problem. The OLE is the solution that minimizes the squared error between the predicted and actual stimuli.\n",
    "\n",
    "In the equation:\n",
    "\n",
    "$ x(t) = x_0 + \\sum_f k(t - t^f) $, \n",
    "\n",
    "$ x(t) $ is the predicted stimulus at time $ t $, $ x_0 $ is a constant, $ k $ is the temporal filter, and $ t^f $ are the observed spike times.\"\n",
    "\n",
    "\n",
    "The temporal filter $ k $ is the heart of the model. It is a function that describes how spike timings are related to the stimulus.\n",
    "\n",
    "To find the optimal temporal filter $ k $, we employ a standard least-squares regression, which minimizes the sum of squared differences between predicted and observed data points.\n",
    "\n",
    "\n",
    "#### Assessing Decoding Uncertainty\n",
    "\n",
    "Assessing decoding uncertainty involves evaluating the posterior variance. Regions of small posterior variance indicate features of the stimulus that are more accurately encoded by the neural response, while large variances point to less certainty.\n",
    "\n",
    "#### Applications in Vision and Neuroprosthetics\n",
    "\n",
    "\"Decoding has significant applications in vision and neuroprosthetics. For instance, reconstructing visual stimuli from neuronal activity can help improve visual prosthetics. Similarly, decoding motor intentions from cortical activity can aid in the development of advanced neuroprosthetic limbs.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from numpy.linalg import inv\n",
    "\n",
    "# Now, let's simulate some spike data and the corresponding stimulus.\n",
    "np.random.seed(42)\n",
    "spike_times = np.sort(np.random.rand(100)) * 100  # 100 random spike times within 100 seconds\n",
    "stimulus_original = np.sin(np.linspace(0, 2 * np.pi, 100))  # Simulate a sinusoidal stimulus\n",
    "\n",
    "# Next, we'll define a function to simulate the application of the temporal filter to the spike times.\n",
    "def apply_temporal_filter(spike_times, stimulus, t):\n",
    "    filtered_stimulus = np.zeros_like(stimulus)\n",
    "    for t_prime in spike_times:\n",
    "        if t_prime < t:\n",
    "            filtered_stimulus += k * stimulus[int(t_prime)]\n",
    "    return filtered_stimulus\n",
    "\n",
    "# We'll use a simple linear filter for k, which we'll define next.\n",
    "k = 1  # For simplicity, we'll start with a filter value of 1.\n",
    "\n",
    "# Applying the filter to the spike times\n",
    "filtered_stimulus = apply_temporal_filter(spike_times, stimulus_original, 100)\n",
    "\n",
    "# Now we'll perform a least-squares regression to optimize the filter k.\n",
    "def linear_model(t, k):\n",
    "    return k * t\n",
    "\n",
    "popt, _ = curve_fit(linear_model, spike_times, filtered_stimulus)\n",
    "\n",
    "# Update the filter with the optimized value\n",
    "k_optimized = popt[0]\n",
    "\n",
    "# Finally, let's plot the original and reconstructed stimuli.\n",
    "plt.plot(stimulus_original, label='Original Stimulus')\n",
    "plt.plot(filtered_stimulus, label='Reconstructed Stimulus with initial k')\n",
    "plt.plot(spike_times, linear_model(spike_times, k_optimized), label='Reconstructed Stimulus with optimized k')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
